{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code snippet from https://www.quora.com/How-can-I-read-a-data-set-of-images-in-a-PNG-format-in-Python-code\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder,filename))\n",
    "        images.append(np.asarray(img))\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 100, 200, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = load_images_from_folder(r\"E:\\Data\\S&P500\\train_images_90in_3out\")\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 100, 200, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"E:\\Data\\S&P500\"\n",
    "file = r\"\\label_90out_3in_3pctchange.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+file, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>close</th>\n",
       "      <th>future_mean</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-05-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1095.449951</td>\n",
       "      <td>1096.473307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1097.280029</td>\n",
       "      <td>1092.079956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-13</th>\n",
       "      <td>1</td>\n",
       "      <td>1096.439941</td>\n",
       "      <td>1090.429972</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1095.699951</td>\n",
       "      <td>1088.090007</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1084.099976</td>\n",
       "      <td>1089.786662</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-18</th>\n",
       "      <td>1</td>\n",
       "      <td>1091.489990</td>\n",
       "      <td>1090.476685</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-19</th>\n",
       "      <td>1</td>\n",
       "      <td>1088.680054</td>\n",
       "      <td>1092.720011</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-20</th>\n",
       "      <td>2</td>\n",
       "      <td>1089.189941</td>\n",
       "      <td>1100.673381</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-21</th>\n",
       "      <td>2</td>\n",
       "      <td>1093.560059</td>\n",
       "      <td>1107.800008</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-24</th>\n",
       "      <td>2</td>\n",
       "      <td>1095.410034</td>\n",
       "      <td>1116.423340</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-25</th>\n",
       "      <td>1</td>\n",
       "      <td>1113.050049</td>\n",
       "      <td>1118.966675</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-26</th>\n",
       "      <td>1</td>\n",
       "      <td>1114.939941</td>\n",
       "      <td>1121.053345</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-27</th>\n",
       "      <td>1</td>\n",
       "      <td>1121.280029</td>\n",
       "      <td>1122.289998</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-28</th>\n",
       "      <td>1</td>\n",
       "      <td>1120.680054</td>\n",
       "      <td>1120.943319</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1121.199951</td>\n",
       "      <td>1121.376668</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1124.989990</td>\n",
       "      <td>1126.520020</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-03</th>\n",
       "      <td>2</td>\n",
       "      <td>1116.640015</td>\n",
       "      <td>1135.033366</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-04</th>\n",
       "      <td>2</td>\n",
       "      <td>1122.500000</td>\n",
       "      <td>1137.976685</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-07</th>\n",
       "      <td>1</td>\n",
       "      <td>1140.420044</td>\n",
       "      <td>1136.659994</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1142.180054</td>\n",
       "      <td>1131.029989</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-09</th>\n",
       "      <td>1</td>\n",
       "      <td>1131.329956</td>\n",
       "      <td>1131.256673</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1136.469971</td>\n",
       "      <td>1130.286703</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1125.290039</td>\n",
       "      <td>1132.540039</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1132.010010</td>\n",
       "      <td>1133.543376</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-16</th>\n",
       "      <td>1</td>\n",
       "      <td>1133.560059</td>\n",
       "      <td>1132.456706</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1132.050049</td>\n",
       "      <td>1133.243368</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-18</th>\n",
       "      <td>1</td>\n",
       "      <td>1135.020020</td>\n",
       "      <td>1136.256714</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-21</th>\n",
       "      <td>1</td>\n",
       "      <td>1130.300049</td>\n",
       "      <td>1139.706706</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-22</th>\n",
       "      <td>1</td>\n",
       "      <td>1134.410034</td>\n",
       "      <td>1139.713379</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-23</th>\n",
       "      <td>1</td>\n",
       "      <td>1144.060059</td>\n",
       "      <td>1136.143351</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>1</td>\n",
       "      <td>2596.639893</td>\n",
       "      <td>2596.390055</td>\n",
       "      <td>3692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>1</td>\n",
       "      <td>2596.260010</td>\n",
       "      <td>2603.003418</td>\n",
       "      <td>3693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>2</td>\n",
       "      <td>2582.610107</td>\n",
       "      <td>2620.786703</td>\n",
       "      <td>3694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>2</td>\n",
       "      <td>2610.300049</td>\n",
       "      <td>2640.923340</td>\n",
       "      <td>3695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>2</td>\n",
       "      <td>2616.100098</td>\n",
       "      <td>2646.523275</td>\n",
       "      <td>3696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>1</td>\n",
       "      <td>2635.959961</td>\n",
       "      <td>2647.436605</td>\n",
       "      <td>3697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>0</td>\n",
       "      <td>2670.709961</td>\n",
       "      <td>2637.976644</td>\n",
       "      <td>3698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>1</td>\n",
       "      <td>2632.899902</td>\n",
       "      <td>2648.596680</td>\n",
       "      <td>3699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>1</td>\n",
       "      <td>2638.699951</td>\n",
       "      <td>2650.313395</td>\n",
       "      <td>3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>1</td>\n",
       "      <td>2642.330078</td>\n",
       "      <td>2649.536703</td>\n",
       "      <td>3701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>1</td>\n",
       "      <td>2664.760010</td>\n",
       "      <td>2654.966716</td>\n",
       "      <td>3702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>2</td>\n",
       "      <td>2643.850098</td>\n",
       "      <td>2675.050049</td>\n",
       "      <td>3703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>2</td>\n",
       "      <td>2640.000000</td>\n",
       "      <td>2697.226725</td>\n",
       "      <td>3704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>2</td>\n",
       "      <td>2681.050049</td>\n",
       "      <td>2711.833415</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>1</td>\n",
       "      <td>2704.100098</td>\n",
       "      <td>2723.033366</td>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>1</td>\n",
       "      <td>2706.530029</td>\n",
       "      <td>2731.393392</td>\n",
       "      <td>3707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>1</td>\n",
       "      <td>2724.870117</td>\n",
       "      <td>2725.120036</td>\n",
       "      <td>3708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>1</td>\n",
       "      <td>2737.699951</td>\n",
       "      <td>2715.180013</td>\n",
       "      <td>3709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>1</td>\n",
       "      <td>2731.610107</td>\n",
       "      <td>2707.909994</td>\n",
       "      <td>3710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07</th>\n",
       "      <td>1</td>\n",
       "      <td>2706.050049</td>\n",
       "      <td>2720.803304</td>\n",
       "      <td>3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>2</td>\n",
       "      <td>2707.879883</td>\n",
       "      <td>2735.853353</td>\n",
       "      <td>3712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>2</td>\n",
       "      <td>2709.800049</td>\n",
       "      <td>2747.829996</td>\n",
       "      <td>3713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>1</td>\n",
       "      <td>2744.729980</td>\n",
       "      <td>2758.120036</td>\n",
       "      <td>3714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>1</td>\n",
       "      <td>2753.030029</td>\n",
       "      <td>2767.030029</td>\n",
       "      <td>3715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>2</td>\n",
       "      <td>2745.729980</td>\n",
       "      <td>2780.020020</td>\n",
       "      <td>3716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>1</td>\n",
       "      <td>2775.600098</td>\n",
       "      <td>2779.779948</td>\n",
       "      <td>3717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-19</th>\n",
       "      <td>1</td>\n",
       "      <td>2779.760010</td>\n",
       "      <td>2784.083252</td>\n",
       "      <td>3718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>1</td>\n",
       "      <td>2784.699951</td>\n",
       "      <td>2787.886637</td>\n",
       "      <td>3719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>1</td>\n",
       "      <td>2774.879883</td>\n",
       "      <td>2794.226644</td>\n",
       "      <td>3720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>1</td>\n",
       "      <td>2792.669922</td>\n",
       "      <td>2794.129964</td>\n",
       "      <td>3721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3722 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            label        close  future_mean  index\n",
       "dates                                             \n",
       "2004-05-11      1  1095.449951  1096.473307      0\n",
       "2004-05-12      1  1097.280029  1092.079956      1\n",
       "2004-05-13      1  1096.439941  1090.429972      2\n",
       "2004-05-14      1  1095.699951  1088.090007      3\n",
       "2004-05-17      1  1084.099976  1089.786662      4\n",
       "2004-05-18      1  1091.489990  1090.476685      5\n",
       "2004-05-19      1  1088.680054  1092.720011      6\n",
       "2004-05-20      2  1089.189941  1100.673381      7\n",
       "2004-05-21      2  1093.560059  1107.800008      8\n",
       "2004-05-24      2  1095.410034  1116.423340      9\n",
       "2004-05-25      1  1113.050049  1118.966675     10\n",
       "2004-05-26      1  1114.939941  1121.053345     11\n",
       "2004-05-27      1  1121.280029  1122.289998     12\n",
       "2004-05-28      1  1120.680054  1120.943319     13\n",
       "2004-06-01      1  1121.199951  1121.376668     14\n",
       "2004-06-02      1  1124.989990  1126.520020     15\n",
       "2004-06-03      2  1116.640015  1135.033366     16\n",
       "2004-06-04      2  1122.500000  1137.976685     17\n",
       "2004-06-07      1  1140.420044  1136.659994     18\n",
       "2004-06-08      1  1142.180054  1131.029989     19\n",
       "2004-06-09      1  1131.329956  1131.256673     20\n",
       "2004-06-10      1  1136.469971  1130.286703     21\n",
       "2004-06-14      1  1125.290039  1132.540039     22\n",
       "2004-06-15      1  1132.010010  1133.543376     23\n",
       "2004-06-16      1  1133.560059  1132.456706     24\n",
       "2004-06-17      1  1132.050049  1133.243368     25\n",
       "2004-06-18      1  1135.020020  1136.256714     26\n",
       "2004-06-21      1  1130.300049  1139.706706     27\n",
       "2004-06-22      1  1134.410034  1139.713379     28\n",
       "2004-06-23      1  1144.060059  1136.143351     29\n",
       "...           ...          ...          ...    ...\n",
       "2019-01-10      1  2596.639893  2596.390055   3692\n",
       "2019-01-11      1  2596.260010  2603.003418   3693\n",
       "2019-01-14      2  2582.610107  2620.786703   3694\n",
       "2019-01-15      2  2610.300049  2640.923340   3695\n",
       "2019-01-16      2  2616.100098  2646.523275   3696\n",
       "2019-01-17      1  2635.959961  2647.436605   3697\n",
       "2019-01-18      0  2670.709961  2637.976644   3698\n",
       "2019-01-22      1  2632.899902  2648.596680   3699\n",
       "2019-01-23      1  2638.699951  2650.313395   3700\n",
       "2019-01-24      1  2642.330078  2649.536703   3701\n",
       "2019-01-25      1  2664.760010  2654.966716   3702\n",
       "2019-01-28      2  2643.850098  2675.050049   3703\n",
       "2019-01-29      2  2640.000000  2697.226725   3704\n",
       "2019-01-30      2  2681.050049  2711.833415   3705\n",
       "2019-01-31      1  2704.100098  2723.033366   3706\n",
       "2019-02-01      1  2706.530029  2731.393392   3707\n",
       "2019-02-04      1  2724.870117  2725.120036   3708\n",
       "2019-02-05      1  2737.699951  2715.180013   3709\n",
       "2019-02-06      1  2731.610107  2707.909994   3710\n",
       "2019-02-07      1  2706.050049  2720.803304   3711\n",
       "2019-02-08      2  2707.879883  2735.853353   3712\n",
       "2019-02-11      2  2709.800049  2747.829996   3713\n",
       "2019-02-12      1  2744.729980  2758.120036   3714\n",
       "2019-02-13      1  2753.030029  2767.030029   3715\n",
       "2019-02-14      2  2745.729980  2780.020020   3716\n",
       "2019-02-15      1  2775.600098  2779.779948   3717\n",
       "2019-02-19      1  2779.760010  2784.083252   3718\n",
       "2019-02-20      1  2784.699951  2787.886637   3719\n",
       "2019-02-21      1  2774.879883  2794.226644   3720\n",
       "2019-02-22      1  2792.669922  2794.129964   3721\n",
       "\n",
       "[3722 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['index'] = df['Unnamed: 0']\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df[\"dates\"] = pd.to_datetime(df[\"dates\"])\n",
    "df = df.set_index('dates')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrain(images, labels_df, year1, year2):\n",
    "    split = labels_df.loc[year1 : year2]\n",
    "    indices = [split['index'][0], split['index'][-1]]\n",
    "    \n",
    "    splitImages = images[indices[0]:indices[1]+1]\n",
    "    splitLabels = split['label']\n",
    "    \n",
    "    return np.array(splitImages), np.array(splitLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = splitTrain(images, df, '2004','2008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1170,), (1170, 100, 200, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[-10])\n",
    "print(train_labels[-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note majority of the signals in the training are sells over 80% which means it was mostly a bearish market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "1  817\n",
       "0  180\n",
       "2  173"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_labels).apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = splitTrain(images, df, '2009','2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1170, 100, 200, 4), (1170,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baselineModel():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 200, 4)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    \n",
    "    #Dense Layers and output\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/37293642/how-to-tell-keras-stop-training-based-on-loss-value\n",
    "class EarlyStoppingByLossVal(callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', value=.951, verbose=0):\n",
    "        super(callbacks.Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"\\nEpoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class score(callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super(score, self).__init__()\n",
    "        self.bestAUC = 0\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = self.model.predict(np.array(self.X_train))[:,[0,2]]\n",
    "        auc = roc_auc_score(to_categorical(self.y_train)[:,[0,2]], pred)\n",
    "\n",
    "        pred = self.model.predict(np.array(self.X_val))[:,[0,2]]\n",
    "        auc_val = roc_auc_score(to_categorical(self.y_val)[:,[0,2]], pred)\n",
    "        print(\"\\nTrain AUC: {} Valid AUC: {}\".format(auc, auc_val))\n",
    "        \n",
    "        if (self.bestAUC < auc_val) :\n",
    "            self.bestAUC = auc_val\n",
    "            self.model.save(\"bestNet.h5\", overwrite=True)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 98, 198, 32)       1184      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 49, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 47, 97, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 23, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 21, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 61824)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                3956800   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 4,013,603\n",
      "Trainable params: 4,013,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = baselineModel()\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1170 samples, validate on 504 samples\n",
      "Epoch 1/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 1.0004 - accuracy: 0.6878\n",
      "Train AUC: 0.6217440003606571 Valid AUC: 0.5670034834196905\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.9927 - accuracy: 0.6906 - val_loss: 1.1276 - val_accuracy: 0.5139\n",
      "Epoch 2/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.8230 - accuracy: 0.6983\n",
      "Train AUC: 0.6936277273787588 Valid AUC: 0.6124604352334329\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.8198 - accuracy: 0.7000 - val_loss: 1.1693 - val_accuracy: 0.5139\n",
      "Epoch 3/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.7819 - accuracy: 0.7009\n",
      "Train AUC: 0.7462262057836837 Valid AUC: 0.5899141034398401\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.7810 - accuracy: 0.7026 - val_loss: 1.3769 - val_accuracy: 0.5119\n",
      "Epoch 4/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.7378 - accuracy: 0.7113\n",
      "Train AUC: 0.802958850732016 Valid AUC: 0.571348732651545\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.7334 - accuracy: 0.7128 - val_loss: 1.4790 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.6606 - accuracy: 0.7400\n",
      "Train AUC: 0.8824911314423733 Valid AUC: 0.5559214462542373\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.6575 - accuracy: 0.7410 - val_loss: 1.1190 - val_accuracy: 0.4921\n",
      "Epoch 6/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.5806 - accuracy: 0.7626\n",
      "Train AUC: 0.9489685226540445 Valid AUC: 0.5542812619039794\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.5790 - accuracy: 0.7632 - val_loss: 1.1660 - val_accuracy: 0.5079\n",
      "Epoch 7/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.8270\n",
      "Train AUC: 0.9814070329846706 Valid AUC: 0.5803997347832972\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.4478 - accuracy: 0.8256 - val_loss: 1.4184 - val_accuracy: 0.5179\n",
      "Epoch 8/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.8748\n",
      "Train AUC: 0.993385424905143 Valid AUC: 0.5596904341395295\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.3149 - accuracy: 0.8726 - val_loss: 1.6973 - val_accuracy: 0.5179\n",
      "Epoch 9/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9200\n",
      "Train AUC: 0.9975388527642834 Valid AUC: 0.5726368157030683\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.2021 - accuracy: 0.9197 - val_loss: 2.2731 - val_accuracy: 0.5198\n",
      "Epoch 10/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9652\n",
      "Train AUC: 0.9995086658189212 Valid AUC: 0.565073283358504\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.1085 - accuracy: 0.9641 - val_loss: 3.5151 - val_accuracy: 0.5139\n",
      "Epoch 11/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9713\n",
      "Train AUC: 0.9998731939901498 Valid AUC: 0.5537381911893086\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.0727 - accuracy: 0.9709 - val_loss: 2.7048 - val_accuracy: 0.4921\n",
      "Epoch 12/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9896\n",
      "Train AUC: 1.0 Valid AUC: 0.5616884376231454\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.0306 - accuracy: 0.9897 - val_loss: 3.7628 - val_accuracy: 0.4980\n",
      "Epoch 13/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9965\n",
      "Train AUC: 1.0 Valid AUC: 0.5603298353120665\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.0096 - accuracy: 0.9966 - val_loss: 4.8215 - val_accuracy: 0.5040\n",
      "Epoch 14/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Train AUC: 1.0 Valid AUC: 0.5642297033736808\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 5.8404 - val_accuracy: 0.5139\n",
      "Epoch 15/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Train AUC: 1.0 Valid AUC: 0.563935582838223\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 6.2276 - val_accuracy: 0.5079\n",
      "Epoch 16/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 8.3685e-04 - accuracy: 1.0000\n",
      "Train AUC: 1.0 Valid AUC: 0.5638377982184688\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 8.2514e-04 - accuracy: 1.0000 - val_loss: 6.5466 - val_accuracy: 0.5079\n",
      "Epoch 17/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 6.3884e-04 - accuracy: 1.0000\n",
      "Train AUC: 1.0 Valid AUC: 0.5633966973065857\n",
      "1170/1170 [==============================] - 5s 5ms/sample - loss: 6.3171e-04 - accuracy: 1.0000 - val_loss: 6.7897 - val_accuracy: 0.5060\n",
      "Epoch 18/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 4.8118e-04 - accuracy: 1.0000\n",
      "Train AUC: 1.0 Valid AUC: 0.5631963682272976\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 4.7788e-04 - accuracy: 1.0000 - val_loss: 7.0112 - val_accuracy: 0.5020\n",
      "Epoch 19/100\n",
      "1150/1170 [============================>.] - ETA: 0s - loss: 3.4689e-04 - accuracy: 1.0000\n",
      "Train AUC: 1.0 Valid AUC: 0.5631859220889355\n",
      "1170/1170 [==============================] - 6s 5ms/sample - loss: 3.4118e-04 - accuracy: 1.0000 - val_loss: 7.2225 - val_accuracy: 0.5020\n",
      "Epoch 20/100\n",
      " 790/1170 [===================>..........] - ETA: 0s - loss: 1.0352e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-fe557a14f813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3217\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 415\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callback = [score(train_images, train_labels, test_images, test_labels)]\n",
    "\n",
    "model.fit(train_images, train_labels,validation_data=(test_images,test_labels) ,batch_size=10,epochs=100, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100, 200, 4), (1,))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =  test_images[240].reshape(1,100,200,4)\n",
    "y = np.array(test_labels[240]).reshape(1,)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model = keras.models.load_model(\"model1.h5\")\n",
    "\n",
    "\n",
    "#model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(x).argmax(axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(\n",
    "            Image.open(\n",
    "                os.path.join(r\"C:\\Users\\ken\\Desktop\\OneDrive\\OneDrive - Knights - University of Central Florida\\UCF Spring 2019\\Machine Learning\\signal\",\n",
    "                             r\"backtest.png\"))).reshape(1,100,200,4)\n",
    "img = img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.1786977e-02, 9.2821276e-01, 1.9995726e-07]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[-0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would have gotten 50% of the calls correct which is to be expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
